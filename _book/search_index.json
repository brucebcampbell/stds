[["index.html", "Selected Topics In Data Science Chapter 1 Preface", " Selected Topics In Data Science Bruce Campbell 2021-01-26 Chapter 1 Preface This is the first installment on my promise to elucidate less popular topics in statistics and machine learning. I wrote this as a way to solidify my understanding of some of the topics that are treated here. Hopefully others will find value here. "],["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction “Where must we go, we who wander this wasteland, in search of our better selves.” -The First History of Man This is a living book. It’s under development. We are using the bookdown package (Xie 2020) in this book, which was built on top of R Markdown and knitr (Xie 2015). "],["on-model-averaging.html", "Chapter 3 On Model Averaging", " Chapter 3 On Model Averaging Recall that we can break down model error into the bias an variance \\(bias(\\hat{Y})= E[\\hat{Y}-E[Y]]\\) If we are averaging models \\(i=1, \\cdots ,k\\) then \\(\\operatorname{MSE}\\left(\\hat{Y}_{i}\\right)=\\left\\{\\operatorname{bias}\\left(\\hat{Y}_{i}\\right)\\right\\}^{2}+\\operatorname{var}\\left(\\hat{Y}_{i}\\right)\\) "],["sensitivity-analysis-and-shapley-values.html", "Chapter 4 Sensitivity Analysis and Shapley Values 4.1 Relationship between Sobol indices and Shapley values 4.2 CRAN sensitivity package 4.3 Partial Correlation Coefficients 4.4 Sobol indices for deterministic function and for model", " Chapter 4 Sensitivity Analysis and Shapley Values Global sensitivity analysis measures the importance of input variables to a function. This is an important task in quantifying the uncertainty in which target variables can be predicted from their inputs. Sobol indices (Saltelli and Sobol’ 1995) are a popular approach to this. It turns out that there’s a relationship between Sobol indices and Shapley values. We explore this relationship here and demonstrate their effectiveness on some linear and non-linear models. 4.1 Relationship between Sobol indices and Shapley values Shapley values are based on \\(f(x)-E[f(x)]\\) while Sobol indices decompose output variance into fractions contributed by the inputs. The Sobol index is a global measure of feature importance while Shapley values focus on local explanations although we could combine local Shapley values to achieve a global importance measure. Sobol indices are based on expectations and can be used for features not included in the model / function of interest. In this way we could query for important features correlated with those that the model does use. 4.2 CRAN sensitivity package library(ggplot2) library(pander) if(!require(sensitivity)){ install.packages(&quot;sensitivity&quot;) library(sensitivity) } Standardized Regression Coefficients (SRC), or the Standardized Rank Regression Coefficients (SRRC), which are sensitivity indices based on linear or monotonic assumptions in the case of independent factors. n &lt;- 100 X &lt;- data.frame(X1 = runif(n, 0.5, 1.5), X2 = runif(n, 1.5, 4.5), X3 = runif(n, 4.5, 13.5)) # linear model : Y = X1 + X2 + X3 y &lt;- with(X, X1 + X2 + X3) Z &lt;- src(X, y, rank = FALSE, logistic = FALSE, nboot = 0, conf = 0.95) pander(Z$SRC,caption = &quot;Standardized Regression Coefficients &quot;) Standardized Regression Coefficients   original X1 0.11 X2 0.3143 X3 0.9739 ggplot(Z, ylim = c(-1,1))+ggtitle(&quot;Standardized Regression Coefficients&quot;) y &lt;- with(X, X1 + X2 + X3) y &lt;- y + rnorm(nrow(X),0,1/2) df&lt;- data.frame(cbind(X,y)) Z &lt;- src(X, y, rank = FALSE, logistic = FALSE, nboot = 0, conf = 0.95) pander(Z$SRC,caption = &quot;Standardized Regression Coefficients &quot;) Standardized Regression Coefficients   original X1 0.09534 X2 0.3344 X3 0.9469 ggplot(Z, ylim = c(-1,1))+ggtitle(&quot;Standardized Regression Coefficients&quot;) #lm.fit = lm(y ~ X1+X2+X3,data = df) #summary(lm.fit) #attach(df) #plot(y, X1+X2+X3) We see how the importance of X3 is ranked above X2 and likewise X2 is more important than X1. This is by design of the simulated data set. The standardized regression coefficients (beta coefficients) are calculated from that has been standardized, let’s normalize and calculate the regression to see if indeed that is the case. dfs&lt;- data.frame(scale(df,center = TRUE,scale = TRUE)) lm.fit = lm(y ~ X1+X2+X3,data = dfs) summary(lm.fit) ## ## Call: ## lm(formula = y ~ X1 + X2 + X3, data = dfs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.43856 -0.12587 -0.01286 0.10897 0.49544 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.405e-16 2.032e-02 0.000 1 ## X1 9.534e-02 2.051e-02 4.648 1.07e-05 *** ## X2 3.344e-01 2.047e-02 16.340 &lt; 2e-16 *** ## X3 9.469e-01 2.055e-02 46.073 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2032 on 96 degrees of freedom ## Multiple R-squared: 0.96, Adjusted R-squared: 0.9587 ## F-statistic: 767.2 on 3 and 96 DF, p-value: &lt; 2.2e-16 We see that the values are very close. 4.3 Partial Correlation Coefficients x &lt;- pcc(X, y, nboot = 100) print(x) ## ## Call: ## pcc(X = X, y = y, nboot = 100) ## ## Partial Correlation Coefficients (PCC): ## original bias std. error min. c.i. max. c.i. ## X1 0.4286396 -0.0055835341 0.085077871 0.2714100 0.6115977 ## X2 0.8576389 0.0011482949 0.023693621 0.8162147 0.9116996 ## X3 0.9781269 0.0003582604 0.003644181 0.9710611 0.9888195 4.4 Sobol indices for deterministic function and for model y.fun &lt;- function(X) { X1&lt;- X[,1] X2&lt;- X[,2] X3&lt;- X[,3] X1+X2+X3 } yhat.fun&lt;-function(X,lm) { X1&lt;- X[,1] X2&lt;- X[,2] X3&lt;- X[,3] yhat &lt;- predict(lm.fit,data.frame(X1=X1,X2=X2,X3=X3)) return(yhat) } nboot = 1000 x &lt;- sobol(model = y.fun, X[1:50,], X[51:100,], order = 3, nboot = nboot) S.sobol &lt;- x$S pander(S.sobol)   original bias std. error min. c.i. max. c.i. X1 0.8568 -0.007486 0.7454 -0.5317 2.38 X2 1.204 -0.008694 0.7607 -0.2973 2.771 X3 1.03 0.003488 0.3163 0.4053 1.628 **X1*X2** -0.8244 0.01486 0.7405 -2.353 0.5788 **X1*X3** -0.8244 0.01486 0.7405 -2.353 0.5788 **X2*X3** -0.8244 0.01486 0.7405 -2.353 0.5788 X1X2X3 0.8244 -0.01486 0.7405 -0.5788 2.353 #yhat.fun(data.frame(X1=1,X2=2,X3=3),lm.fit) x &lt;- sobol(model = yhat.fun,X[1:50,], X[51:100,], order = 3, nboot = nboot) S.sobol &lt;- x$S pander(S.sobol)   original bias std. error min. c.i. max. c.i. X1 0.8498 -0.03261 0.6407 -0.3552 2.173 X2 0.9443 -0.03028 0.6543 -0.2527 2.263 X3 1.186 0.009244 0.09729 0.9695 1.347 **X1*X2** -0.8484 0.03236 0.6402 -2.158 0.3592 **X1*X3** -0.8484 0.03236 0.6402 -2.158 0.3592 **X2*X3** -0.8484 0.03236 0.6402 -2.158 0.3592 X1X2X3 0.8484 -0.03236 0.6402 -0.3592 2.158 "],["random-effects-and-mixed-models.html", "Chapter 5 Random Effects and Mixed Models 5.1 Crossed versus nested random effects. 5.2 Very Large Number of RE’s", " Chapter 5 Random Effects and Mixed Models 5.1 Crossed versus nested random effects. How do they differ and how are they specified correctly in lme4 and in JAGS / Stan? 5.2 Very Large Number of RE’s https://arxiv.org/abs/1610.08088 "],["propensity-score-matching.html", "Chapter 6 Propensity Score Matching 6.1 Caliper", " Chapter 6 Propensity Score Matching 6.1 Caliper Putting constraints on matching can reduce bias (Lunt 2013). Matching on the propensity score is widely used to estimate the effect of an exposure in observational studies. However, the quality of the matches can be affected by decisions made during the matching process, particularly the order in which subjects are selected for matching and the maximum permitted difference between matched subjects (the “caliper”). This study used simulations to explore the effects of these decisions on both the imbalance of covariates and the closeness of matching, while allowing the numbers of potential matches and strengths of association between the confounding variable and the exposure to vary. It was found that, without a caliper, substantial bias was possible, particularly with a relatively small reservoir of potential matches and strong confounder-exposure association. Use of the recommended caliper reduced the bias considerably, but bias remained if subjects were selected by increasing or decreasing propensity score. A tighter caliper led to greatly reduced bias and closer matches, although some subjects could not be matched. This study suggests that a narrow caliper can improve the performance of propensity score matching. In situations where it is impossible to find appropriate matches for all exposed subjects, it is better to select subjects in order of the best available matches, rather than increasing or decreasing the propensity score. "],["introduction-to-normalizing-flows.html", "Chapter 7 Introduction to Normalizing Flows 7.1 Variational Inference With NF 7.2 ", " Chapter 7 Introduction to Normalizing Flows 7.1 Variational Inference With NF Variational inference now lies at the core of large-scale topic models of text (Hoffman et al., 2013), provides the state-of-the-art in semi-supervised classification (Kingma et al., 2014), drives the models that currently produce the most realistic generative models of images (Gregor et al., 2014; Rezende et al., 2014), and are a default Proceedings of the 32 nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&amp;CP volume 37. Copyright 2015 by the author(s). tool for the understanding of many physical and chemical systems. Despite these successes and ongoing advances, there are a number of disadvantages of variational methods that limit their power and hamper their wider adoption as a default method for statistical inference. It is one of these limitations, the choice of posterior approximation, that we address in this paper [http://proceedings.mlr.press/v37/rezende15.pdf] Generative modeling loosely refers to building a model of data, for instance p(image), that we can sample from. This is in contrast to discriminative modeling, such as regression or classification, which tries to estimate conditional distributions such as p(class | image). 7.2 "],["references.html", "References", " References Lunt, Mark. 2013. “Selecting an Appropriate Caliper Can Be Essential for Achieving Good Balance With Propensity Score Matching.” American Journal of Epidemiology 179 (2): 226–35. https://doi.org/10.1093/aje/kwt212. Saltelli, Andrea, and I. M. Sobol’. 1995. “Sensitivity Analysis for Nonlinear Mathematical Models: Numerical Experience.” Matematicheskoe Modelirovanie 7 (January). Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.org/knitr/. ———. 2020. Bookdown: Authoring Books and Technical Documents with r Markdown. https://github.com/rstudio/bookdown. "]]
