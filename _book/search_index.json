[["index.html", "Selected Topics In Data Science Chapter 1 Preface", " Selected Topics In Data Science Bruce Campbell 2020-12-29 Chapter 1 Preface This is the first installment on my promise to elucidate less popular topics in statistics and machine learning. I wrote this as a way to solidify my understanding of some of the topics that are treated here. Hopefully others will find value here. "],["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction This is a living book. It’s under development. We are using the bookdown package (Xie 2020) in this book, which was built on top of R Markdown and knitr (Xie 2015). References "],["on-model-averaging.html", "Chapter 3 On Model Averaging", " Chapter 3 On Model Averaging Recall that we can break down model error into the bias an variance \\(bias(\\hat{Y})= E[\\hat{Y}-E[Y]]\\) If we are averaging models \\(i=1, \\cdots ,k\\) then \\(\\operatorname{MSE}\\left(\\hat{Y}_{i}\\right)=\\left\\{\\operatorname{bias}\\left(\\hat{Y}_{i}\\right)\\right\\}^{2}+\\operatorname{var}\\left(\\hat{Y}_{i}\\right)\\) "],["sensitivity-analysis-and-shapley-values.html", "Chapter 4 Sensitivity Analysis and Shapley Values 4.1 Relationship between Sobol indices and Shapley values 4.2 CRAN sensitivity package 4.3 Partial Correlation Coefficients 4.4 Sobol indices for deterministic function and for model", " Chapter 4 Sensitivity Analysis and Shapley Values Global sensitivity analysis measures the importance of input variables to a function. This is an important task in quantifying the uncertainty in which target variables can be predicted from their inputs. Sobol indices (Saltelli and Sobol’ 1995) are a popular approach to this. It turns out that there’s a relationship between Sobol indices and Shapley values. We explore this relationship here and demonstrate their effectiveness on some linear and non-linear models. 4.1 Relationship between Sobol indices and Shapley values Shapley values are based on \\(f(x)-E[f(x)]\\) while Sobol indices decompose output variance into fractions contributed by the inputs. The Sobol index is a global measure of feature importance while Shapley values focus on local explanations although we could combine local Shapley values to achieve a global importance measure. Sobol indices are based on expectations and can be used for features not included in the model / function of interest. In this way we could query for important features correlated with those that the model does use. 4.2 CRAN sensitivity package library(ggplot2) library(pander) if(!require(sensitivity)){ install.packages(&quot;sensitivity&quot;) library(sensitivity) } Standardized Regression Coefficients (SRC), or the Standardized Rank Regression Coefficients (SRRC), which are sensitivity indices based on linear or monotonic assumptions in the case of independent factors. n &lt;- 100 X &lt;- data.frame(X1 = runif(n, 0.5, 1.5), X2 = runif(n, 1.5, 4.5), X3 = runif(n, 4.5, 13.5)) # linear model : Y = X1 + X2 + X3 y &lt;- with(X, X1 + X2 + X3) Z &lt;- src(X, y, rank = FALSE, logistic = FALSE, nboot = 0, conf = 0.95) pander(Z$SRC,caption = &quot;Standardized Regression Coefficients &quot;) Standardized Regression Coefficients   original X1 0.09815 X2 0.3034 X3 0.9324 ggplot(Z, ylim = c(-1,1))+ggtitle(&quot;Standardized Regression Coefficients&quot;) y &lt;- with(X, X1 + X2 + X3) y &lt;- y + rnorm(nrow(X),0,1/2) df&lt;- data.frame(cbind(X,y)) Z &lt;- src(X, y, rank = FALSE, logistic = FALSE, nboot = 0, conf = 0.95) pander(Z$SRC,caption = &quot;Standardized Regression Coefficients &quot;) Standardized Regression Coefficients   original X1 0.08383 X2 0.3218 X3 0.9117 ggplot(Z, ylim = c(-1,1))+ggtitle(&quot;Standardized Regression Coefficients&quot;) #lm.fit = lm(y ~ X1+X2+X3,data = df) #summary(lm.fit) #attach(df) #plot(y, X1+X2+X3) We see how the importance of X3 is ranked above X2 and likewise X2 is more important than X1. This is by design of the simulated data set. The standardized regression coefficients (beta coefficients) are calculated from that has been standardized, let’s normalize and calculate the regression to see if indeed that is the case. dfs&lt;- data.frame(scale(df,center = TRUE,scale = TRUE)) lm.fit = lm(y ~ X1+X2+X3,data = dfs) summary(lm.fit) ## ## Call: ## lm(formula = y ~ X1 + X2 + X3, data = dfs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.4229 -0.1233 0.0015 0.1277 0.3623 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.467e-16 1.724e-02 0.000 1 ## X1 8.383e-02 1.739e-02 4.821 5.36e-06 *** ## X2 3.218e-01 1.740e-02 18.494 &lt; 2e-16 *** ## X3 9.117e-01 1.736e-02 52.526 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1724 on 96 degrees of freedom ## Multiple R-squared: 0.9712, Adjusted R-squared: 0.9703 ## F-statistic: 1079 on 3 and 96 DF, p-value: &lt; 2.2e-16 We see that the values are very close. 4.3 Partial Correlation Coefficients x &lt;- pcc(X, y, nboot = 100) print(x) ## ## Call: ## pcc(X = X, y = y, nboot = 100) ## ## Partial Correlation Coefficients (PCC): ## original bias std. error min. c.i. max. c.i. ## X1 0.4414781 -0.0037232826 0.092034097 0.2791509 0.6323478 ## X2 0.8836538 0.0006961726 0.014732386 0.8525892 0.9216399 ## X3 0.9830436 -0.0001008586 0.002524996 0.9788997 0.9892227 4.4 Sobol indices for deterministic function and for model y.fun &lt;- function(X) { X1&lt;- X[,1] X2&lt;- X[,2] X3&lt;- X[,3] X1+X2+X3 } yhat.fun&lt;-function(X,lm) { X1&lt;- X[,1] X2&lt;- X[,2] X3&lt;- X[,3] yhat &lt;- predict(lm.fit,data.frame(X1=X1,X2=X2,X3=X3)) return(yhat) } nboot = 1000 x &lt;- sobol(model = y.fun, X[1:50,], X[51:100,], order = 3, nboot = nboot) S.sobol &lt;- x$S pander(S.sobol)   original bias std. error min. c.i. max. c.i. X1 0.9175 -0.004554 0.9386 -0.9498 2.76 X2 0.7255 -0.0121 0.9346 -1.073 2.643 X3 1.065 0.008378 0.3062 0.4374 1.694 **X1*X2** -0.6334 0.01302 0.9601 -2.516 1.245 **X1*X3** -0.6334 0.01302 0.9601 -2.516 1.245 **X2*X3** -0.6334 0.01302 0.9601 -2.516 1.245 X1X2X3 0.6334 -0.01302 0.9601 -1.245 2.516 #yhat.fun(data.frame(X1=1,X2=2,X3=3),lm.fit) x &lt;- sobol(model = yhat.fun,X[1:50,], X[51:100,], order = 3, nboot = nboot) S.sobol &lt;- x$S pander(S.sobol)   original bias std. error min. c.i. max. c.i. X1 0.7179 -0.006484 0.8188 -0.8731 2.418 X2 0.7044 -0.01037 0.8244 -0.8924 2.447 X3 1.265 0.01538 0.1048 1.027 1.435 **X1*X2** -0.696 0.007087 0.8201 -2.4 0.8915 **X1*X3** -0.696 0.007087 0.8201 -2.4 0.8915 **X2*X3** -0.696 0.007087 0.8201 -2.4 0.8915 X1X2X3 0.696 -0.007087 0.8201 -0.8915 2.4 References "],["applications.html", "Chapter 5 Applications 5.1 Example one 5.2 Example two", " Chapter 5 Applications Some significant applications are demonstrated in this chapter. 5.1 Example one 5.2 Example two "],["final-words.html", "Chapter 6 Final Words", " Chapter 6 Final Words We have finished a nice book. "],["references.html", "References", " References "]]
